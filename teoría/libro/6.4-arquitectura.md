# Diseño de la arquitectura

Otra aspecto a tener en cuenta es la estructura de la red neuronal: ¿Cuántas unidades usar, y cómo conectarlas?
Las unidades se agrupan por capas y estas se conectan una tras otra.

<p align="center"><img src="https://icdn5.digitaltrends.com/image/artificial_neural_network_1-791x388.jpg" width="50%"/></p>

Realmente elegimos la estructura de las capas ocultas: ¿Cúantas capas usar? y ¿cuántas unidades en cada capa?
Porque los tamaños de las capas de entrada y de salida es fijo, ya que viene determinado por la función que queremos aproximar,
(qué datos tomamos como entrada, y qué datos producimos como salida. Matemáticamente se dice que se pasa de un espacio finito n-dimensional o otro). Recuerda que la capa de entrada la forman sólo datos, no son perceptrones.

> ### Teorema universal de la aproximación
> **Una sóla capa oculta es suficiente para aproximar cualquer función**, (con la precisión deseada).

Aunque teóricamente una sóla capa oculta sea suficiente, ésta necesitaría muchísimas unidades.
En la práctica se usan varias capas ya que usan muchos menos parámtros para aproximar la misma función.
Pero demasiadas capas implica que la red es más dificil de optimizar.

El teorma se cumple para caulquier función de acctivación (incluidas las ReLUs)

Además aunque una una red neuronal grande sea capaz de aprender cualqueier función,
no implica que su algoritmo de aprendizaje sea capaz de hacerlo. Éste puede fallar por 2 razones:
1. No sea capaz de encontrar los valores de los parámetros que se corresponden con la función a aproximar.
2. El algoritmo elija otra función a consecunecia del overfitting

> ### Nota
> También existe la posibilidad de no usar capa oculta, pero entonces solo podríamos aproximar funciones lineales
> (rectas, planos o hiperplanos). Desfortunadamente la mayoría de las funciones que queremos aproximar son **no-lineales**. 

Así que la arqutectura ideal se suele obtener por prueba y error.
